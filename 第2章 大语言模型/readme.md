# 大语言模型

- [大语言模型](#大语言模型)
  - [大数据+大模型→新智能](#大数据大模型新智能)
  - [大语言模型架构概览](#大语言模型架构概览)
  - [基于 Encoder-only 架构的大语言模型](#基于-encoder-only-架构的大语言模型)
  - [基于 Encoder-Decoder 架构的大语言模型](#基于-encoder-decoder-架构的大语言模型)
  - [基于 Decoder-only 架构的大语言模型](#基于-decoder-only-架构的大语言模型)
  - [非 Transformer 架构](#非-transformer-架构)


## <img src="../figure/star.svg" width="25" height="25" />大数据+大模型→新智能
1.  **Scaling laws for neural language models.** `arXiv`    
    *Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei.* [[PDF](https://arxiv.org/pdf/2001.08361)], 2020.

2.  **Training Compute-Optimal Large Language Models** `arXiv`    
    *Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, Laurent Sifre.* [[PDF](https://arxiv.org/pdf/2203.15556)], 2022.

3.  **PaLM 2 Technical Report.** `arXiv`    
    *Google.* [[PDF](https://arxiv.org/pdf/2305.10403)], 2023.



## <img src="../figure/star.svg" width="25" height="25" />大语言模型架构概览
1.  **Attention is all you need.** `NeurIPS`
    *Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia.* [[PDF](https://arxiv.org/pdf/1706.03762)], 2017.

## <img src="../figure/star.svg" width="25" height="25" />基于 Encoder-only 架构的大语言模型
1.  **A survey on contextual embeddings.** `arXiv`    
    *Qi Liu, Matt J. Kusner, Phil Blunsom.* [[PDF](https://arxiv.org/pdf/2003.07278)], 2020.

2. 


## <img src="../figure/star.svg" width="25" height="25" />基于 Encoder-Decoder 架构的大语言模型



## <img src="../figure/star.svg" width="25" height="25" />基于 Decoder-only 架构的大语言模型



## <img src="../figure/star.svg" width="25" height="25" />非 Transformer 架构
1. **Efficiently modeling long sequences with structured state spaces.** `arXiv`  
   *Albert Gu, Karan Goel, Christopher Ré.* [[PDF](https://arxiv.org/abs/2111.00396)][[Code](https://github.com/state-spaces/s4)], 2021.
2. **On the Parameterization and Initialization of Diagonal State Space Models.** `NeurIPS`  
   *Albert Gu, Karan Goel, Ankit Gupta, Christopher Ré.* [[PDF](https://arxiv.org/abs/2206.11893)], 2022.
3. **RWKV: Reinventing RNNs for the Transformer Era.** `EMNLP`  
   *Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan S. Wind, Stanislaw Wozniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, Rui-Jie Zhu* [[PDF](https://arxiv.org/abs/2305.13048)][[Code](https://github.com/BlinkDL/RWKV-LM)], 2023.
4. **Mamba: Linear-Time Sequence Modeling with Selective State Spaces.** `arXiv`  
   *Albert Gu, Tri Dao.* [[PDF](https://arxiv.org/abs/2312.00752)][[Code](https://github.com/state-spaces/mamba)], 2023.
5. **Learning to (Learn at Test Time): RNNs with Expressive Hidden States.** `arXiv`  
   *Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, et al.* [[PDF](https://arxiv.org/abs/2407.04620)][[Code](https://github.com/test-time-training/ttt-lm-pytorch)], 2024.
