<h5 align="center" style="font-size: larger;"><i>我的语言的界限意味着我的世界的界限。
——维特根斯坦《逻辑哲学论》</i></h5>

# 大模型基础


<div align="center"> 
  <img src=".\figure\cover.png" style="width: 50%">
</div>


## 章节内容

**第一章 语言模型基础[Link]**

语言是一套复杂的符号系统。语言符号通常在音韵（phonology）、词法（morphology）、句法（syntax）的约束下构成，并承载不同的语义（semantics）。语言符号具有不确定性。同样的语义可以由不同的音韵、词法、句法构成的符号来表达；同样的音韵、词法、句法构成的符号也可以在不同的语境下表达不同的语义。因此，语言是概率的。并且，语言的概率性与认知的概率性也存在着密不可分的关系。语言模型（Language Models; LMs）旨在准确预测语言符号的概率。从语言学的角度，语言模型可以赋能计算机掌握语法、理解语义，以完成自然语言处理任务。从认知科学的角度，准确预测语言符号的概率可以赋能计算机描摹认知、演化智能。从 ELIZA 到 GPT-4，语言模型经历了从规则模型到统计模型，再到神经网络模型的发展历程，逐步从呆板的机械式问答程序成长为具有强大泛化能力的多任务智能模型。本章将按照语言模型发展的顺序依次讲解基于统计方法的 n-grams 语言模型、基于循环卷积神经网络（Recurrent Neural Network，RNN）的语言模型，基于 Transformer 的语言模型。此外，本章还将介绍如何将语言模型输出概率值解码为目标文本，以及如何对语言模型的性能进行评估。


**第二章 大语言模型[Link]**


随着数据资源和计算能力的爆发式增长，语言模型的规模和性能得到了质的飞跃。大语言模型（Large Language Model, LLM）凭借着庞大的参数量和丰富的训练数据迅速崛起，正引领着生成式人工智能（Artificial Intelligence Generated Content, AIGC）发展的新潮流。
截至2024年6月，大语言模型的发展呈现出爆炸式增长，各类模型层出不穷，预示着一个由大语言模型驱动的AIGC新时代的到来。这些模型不仅在学术界引起了广泛关注，也在工业界得到了实际应用，展现了其在多种领域的变革潜力。
在本章中，我们将深入探讨大语言模型的相关背景，并分别介绍Encoder-only、Encoder-Decoder以及Decoder-only三种主流架构。通过列举每种架构的典型模型，深入分析它们的模型架构、训练方法以及主要创新之处等。最后，本章还将简单介绍其他一些具有创新性的架构模型，以展现当前语言模型领域的多样性和丰富性。

**第三章 Prompt工程[Link]**

随着模型训练数据规模和参数数量的持续增长，大语言模型突破了泛化瓶颈，并涌现出了强大的指令跟随能力。
泛化能力的增强使得模型能够处理和理解多种未知任务，而指令跟随能力的提升则确保了模型能够准确响应人类的指令。这种能力的结合，使得我们能够通过精心编写的指令输入，即Prompt，来引导模型适应各种下游任务，从而避免了传统微调方法所带来的高昂计算成本。
Prompt工程，作为一门专注于如何编写这些有效指令的技术，成为了连接模型与任务需求之间的桥梁。它不仅要求对模型有深入的理解，还需要对任务目标有精准的把握。通过Prompt工程，我们能够最大化地发挥大语言模型的潜力，使其在多样化的应用场景中发挥出卓越的性能。
本章将深入探讨Prompt工程的概念、方法及作用。我们将介绍上下文学习、思维链等技术，并提供实用的Prompt使用技巧，以及介绍基于Prompt工程的相关应用。通过本章的学习，读者将能深入理解并掌握Prompt工程的精髓，进而在大语言模型的应用和优化中发挥更大的创造力和效率。



**第四章 参数高校微调[Link]**

在现代人工智能的蓬勃发展中，大语言模型以其卓越的性能和广泛的应用场景，成为了众多领域的研究热点。尽管这些模型在大规模数据上的预训练使其具备了丰富的世界知识和多任务能力，但它们在特定领域或任务中的表现仍存在显著的局限性。为了更好地适应新的领域和处理新的任务，通常需要进行模型适配。
然而，传统的模型适配技术通常存在性能不足或计算成本过高等问题。例如，传统的微调方法通常更新模型中的所有参数，需要大量的计算资源和存储空间。因此，在资源受限的环境中，全量微调大语言模型变得不切实际，因此，需要开发参数高效微调技术来克服这些挑战。
本章将深入探讨当前主流的参数高效微调技术，首先将简要介绍参数高效微调的概念和分类学，然后将详细介绍参数高效微调的三类主要方法，包括参数附加方法、参数选择方法和低秩适配方法，探讨它们具体的技术实现和优势。最后，本章还将通过具体案例展示参数高效微调在垂直领域的实际应用。

**第五章 模型编辑[Link]**

在已经训练的大模型中，可能存在偏见、毒性、知识错误等问题。为了纠正这些问题，需要对模型中的特定知识点进行修正，使其能够正确掌握相关内容。在这种情形下，成本高昂的重新预训练不再可行。
虽然微调方法可以有效地向大模型注入新知识，并已被广泛使用，但依然存在训练成本高、可能过拟合以及破坏现有知识等缺点。
为了满足大模型的频繁更新需求，同时规避微调方法的不足，模型编辑应运而生。本章将介绍模型编辑这一新兴技术，第5.1节首先从模型编辑思想、定义、性质等方面作出简介，第5.2节分别从内外两个角度介绍模型编辑经典方法，第5.3节和第5.4节将举例介绍模型编辑的具体方法，最后，第5.5将介绍一些模型编辑的实际应用。

**第六章 检索增强生成[Link]**

在这个信息迅速更迭的时代，数据的价值日益凸显，尽管大语言模型（Large Language Model, LLM）的性能令人惊艳，但其仍存在实时信息更新滞后、易出现幻觉现象等问题。因此，如何高效地从海量数据中提取有用信息以增强模型的性能，成为了众多领域关注的焦点。在此背景下，检索增强生成（Retrieval-Augmented Generation，RAG）应运而生。RAG是一种融合了信息检索与内容生成的创新方法，它通过将检索系统与大型语言模型相结合，旨在实现更精确深入的知识获取和内容生成能力。在本章中，我们将深入探讨RAG系统这一新兴技术。具体而言，我们将首先介绍RAG系统的相关背景、定义以及基本组成，接下来我们将详细介绍RAG系统的常见架构，随后我们将展开讨论RAG系统中知识检索与生成增强部分的技术细节，最后，本章还将简要介绍RAG系统的应用与前景。

## 作者分工

本书作者分工情况如下：

- 第一章作者为：毛玉仁、高云君；
- 第二章作者为：李佳晖、毛玉仁、宓禹；
- 第三章作者为：张超、胡中豪、毛玉仁；
- 第四章作者为：葛宇航、毛玉仁；
- 第五章作者为：宓禹、樊怡江、毛玉仁；
- 第六章作者为：董雪梅、徐文溢、毛玉仁。

高云君为本书编撰总指导。

## 引用

```
@book{FoundationsofLLMs,
  title = {大模型基础},
  author = {毛玉仁 and 高云君 and 李佳晖 and 宓禹 and 张超 and 胡中豪 and 葛宇航 and 樊怡江 and 董雪梅 and 徐文溢},
  year = {2024},
  address = {杭州},
  url = {https://github.com/ZJU-LLMs/Foundations-of-LLMs},
}

```



