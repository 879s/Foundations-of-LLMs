### Maximize Your Data’s Potential: Enhancing LLM Accuracy with Two-Phase Pretraining

*Steven Feng, Shrimai Prabhumoye, Kezhi Kong, Dan Su, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro*

*NVIDIA, Stanford University, Boston University*

现有的大型语言模型预训练方法在数据选择、混合与排序策略上存在局限，通常未能充分考虑不同训练阶段对数据多样性和质量的差异化需求。这种一刀切的策略导致模型在不同阶段难以取得最优表现，影响在下游任务上的准确性和泛化能力。因此，论文提出了一种**两阶段预训练策略**，针对不同训练阶段的特点，分别采用不同的数据选择和混合策略，有效解决了预训练过程中**数据多样性与质量的平衡**以及**不同阶段数据需求不匹配**的问题。

## 研究内容

在模型的预训练阶段，优化数据的选择和混合方案，提升模型在下游任务的表现。

## 研究动机

当前大模型的预训练依赖于大量数据，但关于如何最佳地选择、混合这些数据以提高模型性能这一问题尚未得到很好的解决。

## 技术动机

通过使用分阶段的方式，将模型的训练过程分为两个阶段，来应对模型训练早期和后期对数据不同的需求，从而整体提升模型性能。

## 解决方案


![](https://fastly.jsdelivr.net/gh/bucketio/img1@main/2024/12/27/1735262526407-23fd5c2f-e9f8-4ff0-842d-3be31a613879.png)



论文将模型的预训练过程分为了两个阶段，并且按照不同阶段的侧重点，利用不同的数据选择策略。

**Phase-1:注重数据多样性**
在预训练初期，需要确保模型能够学习广泛的语言特性和泛化能力，因此主要使用多样性更高的数据。论文在这一阶段主要使用Web Crawl来进行训练。因为Web Crawl 数据来源广泛，涵盖了海量的文本内容， 数据涵盖不同领域、风格、语言和上下文，适合在预训练初期帮助模型建立通用的语言理解能力。

![](https://fastly.jsdelivr.net/gh/bucketio/img13@main/2024/12/27/1735262547516-0fc1d381-e0f3-4e94-b875-b5775a59c763.png)

**Phase-2: 质量优先**
在预训练后期，模型已经具备了一定的语言理解能力，需要进一步**细化和优化**。而Web Crawl数据集虽然足够丰富多样，但数据的信息密度往往较低，并且可能包含低质量、重复或有偏见的内容。因此在这一阶段使用高质量、结构化的数据源（如维基百科、代码、数学文本），确保训练后的模型在高复杂度任务上的表现更稳定和准确。


![](https://fastly.jsdelivr.net/gh/bucketio/img9@main/2024/12/27/1735262565634-ea4b7d49-4cf1-4321-955e-a19c2c670809.png)



## 实验结果


![](https://fastly.jsdelivr.net/gh/bucketio/img18@main/2024/12/27/1735262581949-c2b7188c-c96f-4461-b852-413639e03c8d.png)


论文在1T token的规模上设计并验证了数据混合策略， 实验结果显示，该方法在下游任务中平均准确率比随机数据排序和自然分布分别提高了3.4%和17%。接着，论文还进一步扩展到15T token和更大模型规模（25B参数），并证明了方案的有效性。此外，论文还对两阶段中不同的数据集配比等方面进行了深入的探索，为后续大模型的预训练数据选择提供了巨大的参考价值。

---

- 查看 Arxiv 原文请点击"**阅读原文**"[https://arxiv.org/pdf/2412.15285]
- **更多**大模型学习资料，详见浙江大学LLMs Github仓库: 
  https://github.com/ZJU-LLMs/Foundations-of-LLMs
- 本文编辑：李佳晖，毛玉仁