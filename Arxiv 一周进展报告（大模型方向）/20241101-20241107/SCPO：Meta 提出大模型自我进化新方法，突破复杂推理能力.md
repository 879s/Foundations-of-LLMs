# Self-Consistency Preference Optimization

**SCPO：Meta 提出大模型自我进化新方法，突破复杂推理能力 **

**作者**：*Archiki Prasad, Weizhe Yuan等*

**单位**：Meta FAIR, UNC Chapel Hill, New York University


下图给出此文的整体逻辑框架。首先，对文章进行一句话总结，然后简要介绍研究内容、研究动机、技术动机、解决方案以及优势与潜力，以便读者快速了解文章脉络。


![](https://fastly.jsdelivr.net/gh/bucketio/img11@main/2024/11/07/1730984587486-e71370a5-40ed-47bb-be16-0297a6797924.png)


### SCPO 方法



大语言模型（LLMs）已在众多领域展现出卓越的性能，其成功在很大程度上依赖于大规模的人类注释数据。然而，人类注释数据的获取面临着诸多挑战，数据收集过程不仅成本高昂，还需耗费大量时间且要求具备专业知识，这一系列因素构成了制约模型训练进展的关键瓶颈。为应对这一困境，**自训练**方法作为一种极具潜力的解决方案被提出，旨在借助模型自身生成数据来推动性能提升。 

但现有自训练方法暴露出明显的局限性。当面对复杂问题时，模型自身难以准确评估回答的正确性，即便引入外部奖励模型进行回答排序，仍无法有效解决问题，特别是在处理分布外问题时，这些方法的缺陷更为显著。

深入分析可知，复杂推理任务中模型解码过程的随机性虽可能引发随机错误，但同时也为解决问题提供了新视角。在这种情况下，**若多个生成答案趋于一致，则该答案正确的概率显著提高**，类似于在复杂迷宫中多次尝试后路径指向同一方向更可能通向出口的原理。对于**复杂推理任务**（例如算术问题）来说，一个问题的解题过程是多种多样的，但是正确的答案是唯一的，我们可以统计各种答案出现的频率，并以此评估答案的一致性。鉴于此，将答案一致性作为训练过程中衡量正确性的重要指标，为改进自训练方法开辟了新的研究方向。

本篇文章利用自一致性方法的特点，改进了自训练的过程，并且提出了SCPO算法。下面将具体介绍该算法的实现过程。
    
    
### SCPO 算法流程

![](https://fastly.jsdelivr.net/gh/bucketio/img13@main/2024/11/07/1730984626075-6af92593-0840-46c6-92e0-b6f2bb3b5738.png)


算法包括三个流程：

1. **生成新问题并过滤**：SCPO 方法首先**利用模型本身生成多个新的推理问题**。在这个过程中，**基于投票方法对问题进行筛选**。具体而言，对于每个问题，使用模型生成多个响应，然后统计每个答案出现的频率（即投票数）。如果某个问题的所有响应中，没有一个答案的投票数达到预先设定的阈值，则该问题被过滤掉，确保留下来的问题具有一定的质量和可解答性。

2. **标注偏好对**：对于经过筛选后的每个问题，再次使用**温度采样**的方式让模型生成多个响应。接着，通过**投票**函数计算每个响应的最终答案的相对频率，根据投票结果，挑选出**投票数最高（最自一致）**和**最低（最不自一致）**的响应，将它们组成偏好对。这样的偏好对标注方式能够反映模型在不同回答之间的一致性差异，为后续训练提供有价值的信息。

3. **加权改进损失函数并迭代模型**：SCPO **基于模型对偏好对的信心来加权**改进直接偏好优化（DPO）损失函数。具体而言，对于每个偏好对，根据选择和拒绝响应的投票数差异计算实例级权重$\omega(x)$


$$
w(x)=\frac{V(y + ) - V(y⌝)}{k}
$$

  然后将这个权重应用到损失函数中。在迭代训练过程中，模型根据这个加权损失函数不断优化自身参数，从初始模型开始，经过多次迭代（如在实验中通常进行两次迭代），逐步提升模型在复杂推理任务上的性能。每次迭代时，模型都会根据**前一次迭代生成的数据和偏好对**进行训练，使得模型越来越倾向于生成与一致性更高的答案相关的响应。

$$
\mathcal{L}_{ScPO}\left(y^{+}, y^{-} | x\right)=-w(x) \log\sigma\left(\beta\log\frac{M_{\theta}\left(y^{+} | x\right)}{M_{t}\left(y^{+} | x\right)}-\beta\log\frac{M_{\theta}\left(y^{-} | x\right)}{M_{t}\left(y^{-} | x\right)}\right)-\frac{\alpha w(x)}{\left|y^{+}\right|} \log M_{\theta}\left(y^{+} | x\right)
$$


### 实验结果

论文中的实验主要围绕Self-Consistency Preference Optimization（SCPO）方法在多种推理任务上的性能表现展开，具体结果如下：

1. **数学推理任务（GSM8K和MATH数据集）表现**
- **实验设置**：在GSM8K和MATH数据集上分别进行实验，将SCPO方法与多种基线方法对比，包括种子模型（Zero-shot CoT）的贪婪解码和带推理时自一致性（SC）的结果、监督训练方法（IRPO Gold）、无监督训练方法（IRPO RM）等。
    

![](https://fastly.jsdelivr.net/gh/bucketio/img8@main/2024/11/07/1730984881776-13212ba9-dcd4-4ba4-8885-14c72902d501.png)
<center>GSM8K数据集上结果</center>


![](https://fastly.jsdelivr.net/gh/bucketio/img18@main/2024/11/07/1730984938366-50873fa8-63e3-4317-a93d-b3d53d9dc492.png)
<center>MATH数据集上结果</center>


 - **实验结果**

    - **SCPO在无监督设置下表现优异**，在GSM8K数据集上，仅一次迭代的SCPO使用贪婪解码就比零样本种子模型和IRPO RM分别提高了22.74%和12.36%的准确率；在MATH数据集上，两次迭代的SCPO相比相同基线也分别有5.26%和1.64%的提升。
    - **迭代训练对SCPO效果显著**，在GSM8K和MATH数据集上，两次迭代的SCPO在贪婪解码下的准确率均高于一次迭代。例如，在GSM8K上，无监督和半监督训练下准确率分别提高了2.88%和3.03%；在MATH上，M2模型比M1模型贪婪准确率高出最多2.36%。不过，使用8路自一致性计算的准确率在第一次迭代后有时会饱和甚至略微下降，第三次迭代收益也较小。
    - **无监督SCPO与监督训练效果相当**，在GSM8K和MATH数据集上，经过两次迭代训练的SCPO Unsup.在贪婪性能上与IRPO Gold差距小于1%，在n=8的自一致性准确率上差距小于2%。这表明SCPO在不使用真实标签的情况下能达到与监督训练相近的效果。
    - 半监督训练进一步提升性能，在GSM8K数据集上，SCPO Semi-Sup.相比IRPO Gold，贪婪准确率提高了2.35%，SC准确率提高了2.19%；在MATH数据集上也有类似趋势，一次迭代的SCPO Semi-Sup.使用贪婪解码优于IRPO Gold达1.24%。

2. **逻辑推理任务（ZebraLogic数据集）表现**

- **实验设置**：在ZebraLogic数据集上进行实验，同样对比多种基线方法，模型采用相应的初始化设置（如Llama - 3 Instruct 8B作为种子模型），并根据任务特点调整超参数。


![](https://fastly.jsdelivr.net/gh/bucketio/img11@main/2024/11/07/1730984995200-23b4e9cc-7209-464b-9165-8627df64d5f8.png)


- **实验结果**
    - SCPO在无监督设置下显著优于基线，一次迭代的无监督SCPO（M1）使Llama - 3 Instruct 8B种子模型在整体拼图准确率上提高了5.4%，在单元准确率上提高了8.5%；而IRPO RM的无监督训练在该数据集上效果不佳，拼图准确率甚至略有下降。经过两次迭代的SCPO训练，模型在排行榜上提升了8个位置，拼图准确率提高了6.5%，在该数据集上超过了Llama - 3 Instruct 70B、Gemma - 2 27B和Claude - 3 Haiku等更大模型，并且在解决简单拼图问题上有显著提升（高达10.3%），从而获得了最高的单元准确率。

3. **消融实验与分析结果**

    - **加权SCPO损失重要性**：对比使用加权（w(x)基于一致性计算）和未加权（w(x) = 1）的SCPO损失函数进行训练，结果表明加权损失在不同数据集和迭代中均表现更优。在GSM8K和MATH数据集上，第一次迭代时加权损失训练的模型准确率提升明显，分别提高了2.5%和1.44%，即使在第二次迭代，SCPO训练的模型准确率也比未加权的高出约1%。这说明在优化一致性时考虑投票数能更好地反映模型对选择和拒绝标签的信心，有助于提高准确性。
    ![](https://fastly.jsdelivr.net/gh/bucketio/img10@main/2024/11/07/1730985083840-c0078249-46b3-43be-9dfc-7f6b9e9721d4.png)
    - **模型一致性变化**：通过测量使用无监督SCPO训练的模型在不同迭代中最一致响应的投票份额（V(y⁺)/k）来分析模型一致性变化，发现**随着训练迭代增加，模型在不同任务上的一致性均提高**。这可能源于模型准确性提高、偏好优化减少模型多样性以及SCPO训练有效将自一致性分布蒸馏到模型单样本分布等因素。同时，模型在测试准确率较高的任务上更一致，如在GSM8K上最一致且准确，在ZebraLogic上最不一致且准确。
    
	![](https://fastly.jsdelivr.net/gh/bucketio/img19@main/2024/11/07/1730985183311-c9f67137-6da2-4f71-b531-d1c13761b9b2.png)
    - **一致性过滤对构建偏好的影响**：在生成自一致性偏好数据时，对GSM8K和MATH数据集过滤掉少于一半投票指向多数答案的实例（即设定不同阈值τ），分析发现随着阈值增加，训练偏好对的质量提高（如准确率差距增大），但训练数据量减少。实验表明，在MATH数据集上，阈值从0.1k增加到0.7k时，准确率差距从18%增加到68%，但训练数据对数量从6.7K减少到少于700对。当阈值为0.5k时，在数据质量和数量之间达到较好平衡，能提高下游模型性能；而当阈值为0.7k时，数据量可能不足以训练8B参数的模型。
    
	![](https://fastly.jsdelivr.net/gh/bucketio/img13@main/2024/11/07/1730985194544-9cc86ab2-0fdc-44c3-9e38-67964855b33d.png)

    - **自一致性与奖励模型（RM）对比**：通过比较SCPO和使用ArmoRM构建偏好对的IRPO在区分正确和错误响应方面的能力（与黄金标签对比），发现ArmoRM在所有三个数据集上比SCPO有更多错误的成对偏好排序（即选择了错误答案而拒绝了正确答案），这可能是IRPO RM表现较差的主要原因。而自一致性方法虽然会产生更多平局（选择和拒绝答案投票数相同），但在分布外设置（如ZebraLogic）中，自一致性在正确排序成对偏好方面比ArmoRM高12.3%。
    
	![](https://fastly.jsdelivr.net/gh/bucketio/img18@main/2024/11/07/1730985202095-f51366f7-3259-4826-87c2-9977e09fd0e2.png)

    
    
---


- 查看 Arxiv 原文链接请点击“**阅读原文**”
[https://arxiv.org/pdf/2411.04109]
- **更多**模型学习资料，请详见浙大 Daily 实验室 Github 仓库：**https://github.com/ZJU-LLMs/Foundations-of-LLMs**
- 本文编辑：张超 毛玉仁









