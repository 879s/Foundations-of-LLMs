- # Densing Law of LLMs

  Chaojun Xiao , Jie Cai, Maosong Sun等

  _Tsinghua University  ModelBest Inc.等_

  本文引入了“**能力密度（capability density）**”的概念，用于评估LLM的训练质量，同时考虑LLM在性能和效率的发展趋势。通过对近期开源LLM的进一步分析揭示了一条经验定律，即“**密度增长定律（Densing Law）**”：LLM的能力密度随着时间呈指数增长，能力密度大约每**三个月翻一倍**。

  ## 研究内容

  LLM训练质量评估，指综合考虑LLM在性能和效率上的表现。

  ## 研究动机

  Scaling law 揭示LLM性能会随着模型规模的增加而提升。然而，这种扩展带来了巨大的训练和推理效率挑战，变得越来越难以维持。在扩大LLM规模以提高性能与缩小LLM规模以提升效率这两条看似矛盾的路径之间，能否定量评估不同规模LLM的训练质量？

  ## 技术动机

  能提升模型下游任务性能的是模型的有效参数，能够获知模型有效参数规模可以更准确地衡量模型训练质量。

  ## 解决方案

   引入能力密度概念，该密度定义为**有效参数规模与实际参数规模的比值**。

  1. 对于给定模型 $M$，其实际参数规模为 $N_M$，假设其在下游任务上的性能得分为 $S_M$。了计算有效参数规模，本文训练了一系列具有不同参数规模和训练数据规模的参考模型，并基于这些模型拟合参数规模与下游任务性能之间的函数：$S = f(N)$，其中 $S$ 表示下游性能，$N$ 表示参考模型的参数规模。

  2. 在拟合并得到$f$后，利用反函数计算有效参数规模：$\hat{N}(S) = f^{-1}(S)$
  3. 模型 $M$ 的能力密度定义为：$\rho(M) = \frac{\hat{N}(S_M)}{N_M} = \frac{f^{-1}(S_M)}{N_M}$

  ## 实验结果

  ##### 关键发现：LLMs的最大能力密度约每 **3.3个月** 翻一倍，换句话说，每隔约三个月，仅使用一半参数规模的模型即可实现当前最先进LLMs的性能。

  ![](https://fastly.jsdelivr.net/gh/bucketio/img1@main/2024/12/15/1734252142661-de114204-795a-4870-bced-b907a7b70173.png)

  基于上述发现，本文有以下推论：

  1. **推理成本呈指数下降**， 对于性能相当的LLM，其推理成本正在呈指数下降。
  1. **密度定律 × 摩尔定律**，在相同芯片面积上运行的LLM有效参数规模呈指数增长。
  1. **ChatGPT发布后密度增长加速，** ChatGPT发布后，LLM能力密度的增长率提高了50%。
  1. **高效压缩 ≠ 密度提升**， 剪枝和蒸馏方法缩小模型参数的同时，也缩小了模型的能力密度。
  1. **迈向密度最优训练——绿色扩展定律**， 单纯通过增加模型参数以追求性能提升可能导致模型密度下降，并造成不必要的能源消耗，模型开发者需要从单纯优化性能转向优化模型密度。

  综上，本文提出利用能力密度评估模型训练质量，并基于此方法发现大模型的**密度定律**，**模型能力密度随时间呈指数级增长**，大约每三个月翻一倍。

  ---

  - 查看 Arxiv 原文请点击"**阅读原文**"[https://arxiv.org/abs/2410.10630v1]
  - **更多**大模型学习资料，详见浙江大学LLMs Github仓库: 
    https://github.com/ZJU-LLMs/Foundations-of-LLMs
  - 本文编辑：胡中豪，毛玉仁