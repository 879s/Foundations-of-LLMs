### Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs

*Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu*

*Tencent AI Lab, Shanghai Jiao Tong University*

本文聚焦于o1类大语言模型在推理任务中的“过度思考”问题，提出了一种自训练框架，涵盖**长度偏好优化**（SFT、DPO、RPO、SIMPO）和**简化回答**（FCS、GDS）两大策略，分别通过优化推理路径长度和直接简化推理步骤，减少计算冗余并提升推理效率。实验结果表明，该方法在多个基准测试集上有效减少计算开销，同时保持了高水平的性能，展示出在资源受限场景下的广泛应用潜力。

## 研究内容

在o1类大模型的推理过程中经常出现过度思考的问题，导致推理效率较低。例如在回答“2+3等于几”的问题时，o1类的模型（o1-Preview、QwQ-32B-Preview、Deepseek-R1-Preview）会话费过多的计算资源。平均而言，o1类模型花费的Token比常规模型多出1953%。

![](https://fastly.jsdelivr.net/gh/bucketio/img9@main/2025/01/02/1735829768751-75779328-614c-49fc-a3f3-85846750ed79.png)


## 研究动机

当前o1类大模型在面对简单任务时，往往使用与复杂任务相同的推理路径和计算资源，导致资源浪费，推理速度下降，限制了模型在大规模实际场景下的部署和应用。

## 技术动机

通过引入自训练机制，使模型可以学习在简单任务上减少不必要的计算步骤，同时在复杂任务上保持足够的推理深度。

## 解决方案

论文提出了一种基于自训练的优化策略，该策略包括两个核心步骤：

**长度偏好优化（Length Preference Optimization）**



![](https://fastly.jsdelivr.net/gh/bucketio/img12@main/2025/01/02/1735829851877-13247e39-a2b5-4d5b-b32f-08bb16a9030d.png)

首先生成10个样本响应，并丢弃未能生成正确答案的样本，使用不同的策略来选择最终的结果。实验结果发现最短的响应在结果和过程效率方面表现更好，且使用的轮次和令牌更少。由此，基于以下方法来进行长度偏好的优化：

1. **监督式微调（SFT）**：使用正合成数据对预训练模型进行微调，使模型学会将输入映射到优选的输出，通过最小化预测输出和实际输出之间的交叉熵损失。

2. **直接偏好优化（DPO）**：直接在人类偏好的回答上训练模型，增加模型产生优选回答的可能性。
3. **推理偏好优化（RPO）**：在DPO损失上增加一个负对数似然（NLL）项，增强DPO训练的稳定性，同时保持生成内容的期望格式，并防止选定响应的对数概率降低。
4. **简单偏好优化（SimPO）**：解决其他偏好优化方法中奖励函数与生成度量之间的差异，通过自训练方式提升模型效率。

**简化回答（Simplifying Responses to Further Enhance Efficiency）**

尽管较短的响应样本提高了o1类模型的效率，但它们仍然存在“过度思考”问题。通过实验发现，响应中较早的解决方案往往更高效，因此我们进一步简化响应以增强效率。我们提出了三种简化策略，这些策略在如何从开头截断响应方面有所不同。

1. **首次正确解决方案（FCS）**：仅保留最早得出正确答案的解决方案，以减少不必要的后续解决方案。
2. **FCS+Reflection**：在正例中包括第二个达到正确答案的解决方案，以保持模型的长反射能力，同时提高效率。
3. **贪婪多样化解决方案（GDS）**：贪婪地扩展提供新视角的解决方案，增加多样性。


![](https://fastly.jsdelivr.net/gh/bucketio/img14@main/2025/01/02/1735829912263-d9da3ce7-2624-4ebd-8e79-896db799a4fe.png)


## 实验结果

论文的实验结果表明，提出的优化策略在减少推理计算开销的同时，保持了较高的性能表现。在多个基准测试集（如GSM8K、MATH500、GPQA、AIME）上的实验显示，自训练范式（包括长度偏好优化和简化回答）有效减少了推理过程中的冗余步骤，提高了推理效率。此外，在简单任务上，模型表现出更快的响应速度，而在复杂任务上，性能几乎没有下降。这验证了所提出方法在资源受限场景中的实际应用潜力。

![](https://fastly.jsdelivr.net/gh/bucketio/img4@main/2025/01/02/1735829815990-48607a84-ddc4-49e2-8867-a99ef65fd9bf.png)

---

- 查看 Arxiv 原文请点击"**阅读原文**"[https://arxiv.org/pdf/2412.21187]
- **更多**大模型学习资料，详见浙江大学LLMs Github仓库: 
  https://github.com/ZJU-LLMs/Foundations-of-LLMs
- 本文编辑：李佳晖，毛玉仁