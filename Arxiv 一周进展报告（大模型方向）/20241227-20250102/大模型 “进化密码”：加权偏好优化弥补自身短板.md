# Plug-and-Play Training Framework for Preference Optimization	



**作者**：*Jingyuan Ma , Rui Li等*

**单位**：北京大学, 北京航空航天大学


本文提出一种即插即用的加权偏好优化训练框架，通过动态调整训练样本的权重，优化大语言模型在偏好优化中的表现。

## 研究内容

构建加权偏好优化训练框架。通过多次采样分析模型的输出分布，为不同难度的训练样本分配权重，并将这些权重整合到偏好优化过程中，提升模型性能。


## 研究动机

现有方法（如DPO、PPO）对所有样本同等对待，忽略了样本复杂性和模型错误倾向的差异，这种对所有样本一视同仁的做法导致模型对于困难样本的学习效果不佳。




## 技术动机

模型的输出分布可以反映其对问题的掌握程度，简单问题输出一致，复杂问题输出多样且错误率高。通过多次采样，分析模型在不同问题上的表现，动态调整样本权重，优先处理模型难以掌握的问题。


## 解决方案

![](https://fastly.jsdelivr.net/gh/bucketio/img8@main/2025/01/02/1735830966010-369d73bd-8d1b-4983-ad60-7fc98b01d341.png)


该框架通过动态调整训练样本的权重，优先处理模型难以掌握的困难样本，从而提升模型在偏好优化中的表现。具体分为以下三个步骤：

#### 1. **数据收集**
   - **多次采样**：对每个问题多次采样模型的输出，收集模型的响应分布。通过多次采样，能够更全面地了解模型在不同问题上的表现，尤其是模型在简单问题和复杂问题上的输出差异。
   - **数据集构建**：将多次采样的结果构建成数据集 $ D = \{x, y_1, y_2, \ldots, y_n\} $，其中 $ x $ 是问题，$ y_1, y_2, \ldots, y_n $ 是模型的不同响应。
   

#### 2. **权重计算**
   - **分析输出分布**：通过分析模型在多次采样中的输出分布，计算每个问题的正确响应和错误响应的频率。具体来说，计算正确响应的数量 $ P_c $ 和错误响应的数量 $ P_e $。
   - **动态权重分配**：根据模型的表现，为每个问题分配权重。权重的计算公式如下：
  
  
$$
     w = \begin{cases} 
     1 + \alpha \cdot \frac{P_e}{N}, & \text{if } P_c = 0, \\
     \max \left(1, 1 + \alpha \cdot \frac{P_e}{P_c + \epsilon} \cdot \frac{1}{N}\right), & \text{if } P_c > 0.
     \end{cases}
$$

其中，$ \alpha $ 是控制权重调整幅度的超参数，$ \epsilon $ 是一个小常数，$ N $ 是每个问题的采样次数。该公式确保模型在频繁出错的样本上获得更高的权重，从而在训练中优先处理这些困难样本。

   - **构建偏好数据对**：根据权重计算结果，构建偏好数据对 $ D = \{x, y_w, y_l\} $，其中 $ y_w $ 是模型生成的正确响应，$ y_l $ 是错误最多的响应。如果模型未能生成正确响应，则使用标准答案作为 $ y_w $。
   
   
#### 3. **加权训练**
   - **整合权重到优化目标**：将计算得到的权重整合到偏好优化过程中，使用 Bradley-Terry 模型优化目标函数。具体来说，优化目标为：
   
$$
     L_R = -\mathbb{E}_{(x, y_w, y_l) \sim D} \left[ \log \sigma \left( w \cdot (r(x, y_w) - r(x, y_l)) \right) \right]
$$

其中，$ r(x, y_w) $ 和 $ r(x, y_l) $ 分别是模型对正确响应和错误响应的评分，$ w $ 是样本的权重，$ \sigma $ 是 sigmoid 函数。通过引入权重，模型在训练过程中会优先优化那些权重较高的困难样本。在实际训练中，权重的引入仅增加了少量的计算开销，能够在不显著增加训练时间的情况下，提升模型的训练效果。


## 实验结果

为了验证提出的加权偏好优化训练框架的有效性，作者进行了多项实验，主要围绕数学推理任务展开，使用了多个数据集和模型进行测试。在 GSM8K 和 MATH500 数据集上的实验结果表明，加权训练框架显著提升了模型的性能。例如，Qwen2-7B-Instruct 模型在 MATH500 数据集上的准确率从 51.0% 提升至 57.6%。加权训练框架在多种偏好优化方法（如 DPO、DPOP、SimPO、IPO）中均表现出色，证明了其即插即用的特性。


![](https://fastly.jsdelivr.net/gh/bucketio/img15@main/2025/01/02/1735830946188-637e7a17-b29c-447c-9318-713cf4472da2.png)





---


- 查看 Arxiv 原文链接请点击“**阅读原文**”
[https://arxiv.org/pdf/2412.20996]
- **更多**模型学习资料，请详见浙大 Daily 实验室 Github 仓库：**https://github.com/ZJU-LLMs/Foundations-of-LLMs**
- 本文编辑：张超 毛玉仁




