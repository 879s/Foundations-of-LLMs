# Think&Cite: Improving Attributed Text Generation with Self-Guided Tree Search and Progress Reward Modeling

*Junyi Li, Hwee Tou Ng*

Department of Computer Science, National University of Singapore



本文提出**Think&Cite**框架，将带引用文本生成任务转化为多步骤推理问题，设计自引导蒙特卡洛树搜索（SG-MCTS）方法，利用大语言模型的自我反思能力优化推理路径，并引入过程奖励模型（PRM）从生成过程和引用过程两个方面为搜索过程提供反馈，从而提升生成文本的准确性和引用质量。

## 研究内容

改进大语言模型生成带引用文本的能力，逐步生成文本并引用相关文献，确保生成的文本和引用的一致性，使生成内容更加可靠。

## 研究动机

现有方法通过 Prompt方法或监督微调让 LLM 在生成文本时提供引用，它们完全基于自回归过程，任何中间生成错误（例如，错误陈述或错误引用）都可能导致最终回答不正确。

## 技术动机

将带引用文本生成任务转化为多步骤推理问题，将搜索算法引入带引用文本生成过程，从而避免错误的推理路径。

## 解决方案

![](https://fastly.jsdelivr.net/gh/bucketio/img3@main/2024/12/22/1734851975703-38a3eccd-a171-4d4f-bc1d-a717a8b59999.png)

1. **自引导的蒙特卡洛树搜索（Self-Guided Monte Carlo Tree Search, SG-MCTS）**：

   扩展了经典的蒙特卡洛树搜索（MCTS），利用 LLM 的自我反思能力来检查 MCTS 的中间状态，并指导树扩展过程，主动避免推理路径上的错误，其具体步骤为：

   - **选择**：使用UCT算法选择最优节点进行扩展。
   - **扩展**：利用 LLM 的自我反思能力检查并修正查询关键词，从语料库中检索相关文献，通过迭代**思考-表述-引用过程**生成高质量的子节点文本。
   - **评估**：使用进度奖励模型计算新扩展节点的预期奖励。
   - **反向传播**：将新节点的奖励回传到其父节点，更新路径上每个节点的值函数。

2. **过程奖励模型（Progress Reward Models, PRM）**

   引入进度奖励模型来衡量从根节点到当前状态的树搜索进度，包括生成过程奖励和归因过程奖励两个方面。提供了可靠和全面的反馈，以指导MCTS搜索过程。

   - **生成过程奖励**：衡量生成文本的质量，通过现有的**经过直接偏好优化（DPO）的模型**来计算生成句子的质量得分。
     $$
     R_g(\boldsymbol{y}_{1:t+1})=\sum_{k=0}^tw_k\log\frac{\pi^*(y_{k+1}|\boldsymbol{x},\boldsymbol{y}_{1:k})}{\pi_{\mathrm{ref}}(y_{k+1}|\boldsymbol{x},\boldsymbol{y}_{1:k})},
     $$

   - **归因过程奖励**：使用自然语言推理（NLI）模型判断引用的文献是否能够支持生成的句子，通过**引用召回率**和**引用精确率**来评估引用质量。

3. **迭代思考-表述-引用（think-verbalize-cite）范式**

   在MCTS的扩展过程中，框架通过迭代**"思考-表述-引用"**的过程来生成带引用文本，即在每一步中生成一个句子，并在每个句子中引用支持该句子的文献。

   

## 实验结果

实验表明，**Think&Cite** 在多个数据集上提升了生成带引用文本的准确性和可靠性。在需要复杂推理和引用支持的任务中，**Think&Cite** 比传统方法表现更优，显著减少了生成过程中的错误内容和无效引用。

![](https://fastly.jsdelivr.net/gh/bucketio/img6@main/2024/12/22/1734852007012-4e580144-c5d3-4621-b39a-f7a24a1a7d6c.png)

综上，**Think&Cite** 引入了一种新的生成带引用文本的范式，通过结合蒙特卡洛树搜索和过程奖励模型，克服了传统自回归生成方法的局限性。多步骤推理和动态反思机制使得 **Think&Cite** 在处理复杂生成任务时，能够更灵活地进行推理，并在生成质量和引用一致性上超越了现有的方法。

------

- 查看 Arxiv 原文请点击"**阅读原文**"[https://arxiv.org/abs/2412.14860]
- **更多**大模型学习资料，详见浙江大学LLMs Github仓库: https://github.com/ZJU-LLMs/Foundations-of-LLMs
- 本文编辑：樊怡江，毛玉仁

