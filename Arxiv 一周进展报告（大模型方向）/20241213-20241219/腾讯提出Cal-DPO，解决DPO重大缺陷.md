# Cal-DPO: Calibrated Direct Preference Optimization for Language Model Alignment



**作者**：*Teng Xiao, Yige Yuan等*

**单位**：Pennsylvania State University, Tencent AI Lab等


本文提出了 Cal-DPO 算法，通过校准隐式奖励与真实奖励的尺度，解决在对比偏好优化训练过程中正例响应的奖励值持续下降问题，显著提升了大语言模型在人类偏好对齐任务中的表现。

## 研究内容

改进现有对比偏好优化方法奖励机制，提升模型在人类偏好对齐任务中的表现。



## 研究动机

现有对比偏好优化方法（如DPO）主要关注于提升正负例隐式奖励值之差，忽略真实奖励的绝对值，真实奖励与隐式奖励存在尺度不一问题，导致在训练过程中正例响应的奖励值持续下降，从而使得对于正例的似然概率降低，这意味着模型对正例的置信度降低，进而增大模型在推理和数学问题的不确定性，导致生成的结果不够准确或可靠。

如下图所示，***Chosen、Rejected、Margins***分别代表正例响应的奖励值、负例响应的奖励制、正负例响应的奖励制之差。可以看到，在DPO中，随着训练进程推进，尽管正负例响应的奖励值之差不断增大，但是正例的响应值持续下降。而真实应用中，希望奖励模型能够给予正例更高的奖励值，DPO算法构造的奖励模型与真实的奖励值存在偏差，进而影响模型性能。


![](https://fastly.jsdelivr.net/gh/bucketio/img16@main/2024/12/22/1734847233759-172a1b6b-cfb0-4408-9305-b51ae2654787.png)

## 技术动机

上述现象出现的原因在于，DPO的损失函数中只针对正负例响应的隐式奖励奖励值之差进行建模，忽略了正例本身的真实奖励。为了防止正例奖励随训练进程下降，需要校准隐式奖励与真实奖励的尺度，确保隐式奖励与真实奖励在同一尺度上。

## 解决方案

为解决原始DPO算法的对比性损失无法未校准奖励本身尺度的问题，定义校准损失：

$$
\mathcal{L}_{Cal}(\theta; x, y)=(log\frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x)}-\frac{r(x, y)}{\beta})^{2}
$$


其中$r(x, y)$是真实奖励，$\beta$是超参数。在仅有时对偏好反馈的场景中，定义$r(x, y_w)=\frac{1}{2}$，$r(x, y_l)=-\frac{1}{2}$。

结合校准损失与基于BT模型的DPO损失，得到最终Cal-DPO损失为：



![](https://fastly.jsdelivr.net/gh/bucketio/img19@main/2024/12/22/1734847260392-7c2a5b8c-35fe-4d67-aee2-8109b20bc78e.png)

上述公式中蓝色为校准损失，$$\beta$$为超参数，直观上来看，该损失期望正例响应奖励值朝着$$1/2\beta$$靠近，负例响应奖励值朝着$$-1/2\beta$$靠近，使得正例响应奖励值增加，负例奖励值降低，从而使模型生成的响应更符合人类偏好。

## 实验结果

Cal-DPO 在多个方面的实验结果展现出其显著优势与有效性，在多个关键指标和任务场景下均表现出色，优于已有的对比偏好优化算法。


![](https://fastly.jsdelivr.net/gh/bucketio/img11@main/2024/12/22/1734847289107-fb543817-f56b-413e-93fe-b1ae6f0a51e6.png)


![](https://fastly.jsdelivr.net/gh/bucketio/img13@main/2024/12/22/1734847295437-080ec217-7cac-4ab9-b072-d783f42da892.png)

论文还通过实验展示了 Cal-DPO 在训练过程中奖励值的变化情况，与 DPO 进行对比，突出了 Cal-DPO 在维持正例奖励值和调整负例奖励值方面的优势。在Cal-DPO下，正例响应的奖励持续增加且保持正值，这显示 Cal-DPO 有效地推动了正例响应的奖励朝着期望的方向发展，使得模型更倾向于生成符合人类偏好的总结内容。而在DPO下，负例响应的奖励不断下降，并出现了下降到零以下的情况。


![](https://fastly.jsdelivr.net/gh/bucketio/img9@main/2024/12/22/1734847306680-8fd797f7-597c-40c7-a3bc-ab0b7ad27f1f.png)

---


- 查看 Arxiv 原文链接请点击“**阅读原文**”
[https://arxiv.org/pdf/2412.14516]
- **更多**模型学习资料，请详见浙大 Daily 实验室 Github 仓库：**https://github.com/ZJU-LLMs/Foundations-of-LLMs**
- 本文编辑：张超 毛玉仁




