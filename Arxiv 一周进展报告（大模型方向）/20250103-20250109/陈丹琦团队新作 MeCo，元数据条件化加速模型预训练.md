# Metadata Conditioning Accelerates Language Model Pre-training

*Tianyu Gao, Danqi Chen 等*

*Princeton Language and Intelligence, Princeton University*

本文提出了一种新的语言模型预训练方法——**元数据调节后冷却**（Metadata Conditioning then Cooldown，简称 MeCo）。该方法通过在预训练过程中将元数据（如 URL）与文本一起提供，提升了训练效率和模型的可控性。实验表明，与标准预训练方法相比，MeCo 能以更少的数据和计算资源实现更好的下游任务性能，同时能够通过元数据引导模型生成更合适的内容。

## 研究内容

该研究探讨了一种高效的语言模型预训练加速方法，即元数据调节后冷却（MeCo），以提升模型训练效率和生成内容的可控性。

## 研究动机

现有方法未能充分利用预训练语料中的多样性信息，尤其是元数据（如来源 URL）。这些方法将所有数据等同对待，忽略了数据来源的上下文信号，导致模型无法准确体现数据的特定行为，进而影响下游任务的表现。

## 技术动机

通过在预训练初期引入元数据增强模型对数据来源的感知能力。这一策略能有效加速预训练过程，并增强语言模型的可引导性。

## 解决方案

![](https://fastly.jsdelivr.net/gh/bucketio/img10@main/2025/01/13/1736735080103-b8345033-8ed1-4134-8c28-7e356fc70bbb.png)

MeCo 方法由两个阶段组成，分别是**元数据条件训练阶段**和冷却阶段。两个阶段的有机结合在提升预训练效率的同时，保留了模型在推理阶段的通用性。

### 元数据条件训练阶段（初始 90%）

**元数据条件训练阶段**是 MeCo 的核心部分，贯穿了预训练的初始 90% 阶段。在这一阶段，每个训练样本都通过将元数据（如文档的来源 URL）附加到文本前部进行标记。例如，对于来自 Wikipedia 的文本，元数据可能会以 `URL: en.wikipedia.org\n\n[文本内容]` 的形式添加。这一设计使得模型能够学习到不同来源数据之间的语境差异，从而提升对数据多样性的理解能力。在训练过程中，为避免元数据直接干扰模型的语言建模能力，仅对文本内容部分的 token 计算损失，而忽略元数据部分的损失。

### 冷却阶段（最后 10%）

虽然在训练中引入元数据大大提升了模型对不同数据来源的理解能力，但如果模型始终依赖元数据进行推理，可能会限制其在实际应用中的灵活性。因此，**MeCo 在训练的最后阶段切换为标准的文本输入**。这一冷却阶段确保了模型在推理时，即使没有元数据的帮助，也能做出合理的预测。

冷却阶段直接继承了元数据条件阶段的**优化器状态**和**学习率衰减**策略。

### 为什么 MeCo 有效？

MeCo 的核心成功因素在于其利用元数据将语料中的内容分组。例如，元数据帮助模型理解某些来源更倾向于产生高质量的知识性文本，而其他来源则可能包含较多噪声。通过这样的分组，模型能够更高效地利用训练数据。此外，即使使用随机生成的元数据（如散列的 URL），模型也能获得类似的提升，这**表明元数据的语义本身并非关键**，而是**它所提供的分组信号**起到了主要作用。

## 实验结果

文章使用了 **C4、RefinedWeb 和 DCLM** 等数据集验证 MeCo 方法的有效性，模型的训练规模包括 **600M, 1.6B, 3B 以及 8B 参数**。与标准预训练方法相比，MeCo 方法在使用 **33% 更少**的训练数据的情况下，达到了相当的下游任务性能。例如下表，在 **1.6B 参数**模型上，MeCo 方法平均提升了多个下游任务的性能指标，同时显著减少了计算开销。

![](https://fastly.jsdelivr.net/gh/bucketio/img13@main/2025/01/13/1736735108662-be7312bd-ff90-48d5-a137-3d5c6ccb9954.png)

此外，MeCo 方法还显示出其在推理阶段的独特优势。通过在推理时引入元数据作为条件输入（Conditional Inference），模型能够实现对生成内容的精确控制。例如，添加 `wikipedia.org` 作为元数据可以**显著减少有害内容**的生成，而使用虚构的 URL（如 `factquizmaster.com`）则可以**提升模型在常识问答任务中的表现**。实验进一步表明，与标准预训练模型相比，MeCo 模型在带有条件输入时的性能提升更为显著。

**有毒内容生成**：

![](https://fastly.jsdelivr.net/gh/bucketio/img7@main/2025/01/13/1736735132837-af622ad7-38c7-482a-ba78-6518c140e555.png)

**常识问答**：

![](https://fastly.jsdelivr.net/gh/bucketio/img4@main/2025/01/13/1736735146179-c0ba958b-fdbf-4342-bf85-c989af327a25.png)

综上，MeCo 是一种简单而有效的预训练方法，通过元数据条件和冷却阶段相结合，显著提升了数据利用效率，并增强了模型的可控性。它不仅适用于不同规模的模型（从 600M 到 8B 参数），还能够兼容多种元数据类型，甚至包括由模型生成的主题标记。

- 查看 Arxiv 原文请点击"**阅读原文**"[https://arxiv.org/abs/2501.01956]
- **更多**大模型学习资料，详见浙江大学LLMs Github仓库: 
  https://github.com/ZJU-LLMs/Foundations-of-LLMs
- 本文编辑：葛宇航，毛玉仁